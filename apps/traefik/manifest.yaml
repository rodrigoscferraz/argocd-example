apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: traefik
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "-1"

  # Add this finalizer ONLY if you want these to cascade delete (A cascade delete, deletes both the app and its resources, rather than only the app.)
  finalizers:
    - resources-finalizer.argocd.argoproj.io

spec:
  project: default
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
      - PrunePropagationPolicy=foreground
      # - ServerSideApply=true # Necessary due to some resources that are created dinamically and not by th ehelm chart https://github.com/prometheus-community/helm-charts/issues/2196#issuecomment-1713701019
      # - PruneLast=true # Necessary due to some resources that are created dinamically and not by th ehelm chart https://github.com/prometheus-community/helm-charts/issues/2196#issuecomment-1713701019

  source:
    chart: traefik
    repoURL: "https://traefik.github.io/charts"
    targetRevision: 26.1.0
    # https://github.com/traefik/traefik-helm-chart/blob/645fbae55c013870c26fd7fcfecf909465ca0e76/traefik/values.yaml#L1
    helm:
      # skipCrds: true
      valuesObject:
        deployment:
          labels:
            prometheus.io/scrap-with: kube-prometheus-stack
          podLabels:
            prometheus.io/scrap-with: kube-prometheus-stack
        logs:
          general:
            level: DEBUG
        ingressRoute:
          dashboard:
            # -- Create an IngressRoute for the dashboard
            enabled: true
            # -- Additional ingressRoute annotations (e.g. for kubernetes.io/ingress.class)
            annotations: {}
            # -- Additional ingressRoute labels (e.g. for filtering IngressRoute by custom labels)
            labels: {}
            # -- The router match rule used for the dashboard ingressRoute
            matchRule: Host(`localhost`) && PathPrefix(`/dashboard`) || PathPrefix(`/api`)
            entryPoints: ["web"]

        ports:
          # The name of this one can't be changed as it is used for the readiness and
          # liveness probes, but you can adjust its config to your liking
          traefik:
            port: 9000
            expose: true
            # The exposed port for this service
            exposedPort: 9000
            # The port protocol (TCP/UDP)
            protocol: TCP

            #
            # Things that I felt more comfortable setting via the CLI
            #
            additionalArguments:
              - "--api.insecure=true"
          metrics:
            # -- When using hostNetwork, use another port to avoid conflict with node exporter:
            # https://github.com/prometheus/prometheus/wiki/Default-port-allocations
            port: 9001
            # hostPort: 9100
            # Defines whether the port is exposed if service.type is LoadBalancer or
            # NodePort.
            #
            # -- You may not want to expose the metrics port on production deployments.
            # If you want to access it from outside your cluster,
            # use `kubectl port-forward` or create a secure ingress
            expose: false
            # -- The exposed port for this service
            exposedPort: 9001
            # -- The port protocol (TCP/UDP)
            protocol: TCP
            # -- Defines whether the port is exposed on the internal service;
            # note that ports exposed on the default service are exposed on the internal
            # service by default as well.
            exposeInternal: false
        metrics:
          prometheus:
            serviceMonitor:
              metricRelabelings:
                - sourceLabels: [__name__]
                  separator: ;
                  regex: ^fluentd_output_status_buffer_(oldest|newest)_.+
                  replacement: $1
                  action: drop
              relabelings:
                - sourceLabels: [__meta_kubernetes_pod_node_name]
                  separator: ;
                  regex: ^(.*)$
                  targetLabel: nodename
                  replacement: $1
                  action: replace
              jobLabel: traefik
              interval: 30s
              honorLabels: true
              additionalLabels:
                prometheus.io/scrap-with: kube-prometheus-stack
            prometheusRule:
              additionalLabels:
                prometheus.io/scrap-with: kube-prometheus-stack
              rules:
                - alert: TraefikDown
                  expr: up{job="traefik"} == 0
                  for: 5m
                  labels:
                    context: traefik
                    severity: warning
                  annotations:
                    summary: "Traefik Down"
                    description: "{{ $labels.pod }} on {{ $labels.nodename }} is down"

  destination:
    server: "https://kubernetes.default.svc"
    namespace: traefik
